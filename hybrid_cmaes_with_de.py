import numpy as np
import torch
from deap import base, creator, tools, cma
import os
import datetime
import logging
import random
class HybridCMAESWithDE:
    def __init__(self, min_weight, max_weight, last_layer_shape, validation_class, de_rate=0.2, resume_from_checkpoint=False):
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.last_layer_shape = last_layer_shape
        self.validation_class = validation_class
        self.de_rate = de_rate  # Fraction of individuals for DE refinement
        self.resume_from_checkpoint = resume_from_checkpoint  # Whether to resume from a checkpoint
        self.logger = logging.getLogger(__name__)
        # DEAP Setup
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
        creator.create("Individual", list, fitness=creator.FitnessMax)

        self.toolbox = base.Toolbox()
        self.toolbox.register("attr_float", lambda: np.random.uniform(self.min_weight, self.max_weight))
        self.toolbox.register(
            "individual", tools.initRepeat, creator.Individual, self.toolbox.attr_float, n=np.prod(last_layer_shape)
        )
        self.toolbox.register("population", tools.initRepeat, list, self.toolbox.individual)
        self.toolbox.register("evaluate", self.fitness_function)
        self.toolbox.register("mate", tools.cxSimulatedBinary, eta=5.0)
        # CMA-ES Initialization with Uniform Distribution
        self.cma_strategy = cma.Strategy(
            centroid=np.random.uniform(self.min_weight, self.max_weight, np.prod(last_layer_shape)).tolist(),
            sigma=0.7,
            lambda_=100
        )
        self.toolbox.register("generate", self.cma_strategy.generate, creator.Individual)
        self.toolbox.register("update", self.cma_strategy.update)

        # Stagnation tracking
        self.best_fitness_so_far = -float("inf")
        self.no_improvement_count = 0

    def fitness_function(self, individual):
        individual_tensor = torch.FloatTensor(individual).reshape(self.last_layer_shape).to(self.validation_class.device)
        accuracy = self.validation_class.validate_model_with_weights(individual_tensor)
        return accuracy,

    def calculate_diversity(self, population):
        """Calculate population diversity as the mean standard deviation across genes."""
        population_array = np.array([ind for ind in population])
        return np.mean(np.std(population_array, axis=0))

    def reintroduce_diversity_with_strategies(
    self, 
    population, 
    diversity_threshold=0.01, 
    mutate_fraction=0.3, 
    crossover_fraction=0.4, 
    sampling_fraction=0.3, 
    elitism_fraction=0.2, 
    tournsize=3
):
        """
        Reintroduce diversity using a mix of aggressive mutation, sampling, and crossover strategies.

        Args:
            population (list): Current population.
            diversity_threshold (float): Threshold for triggering diversity reintroduction.
            mutate_fraction (float): Fraction of elites to mutate aggressively.
            crossover_fraction (float): Fraction of individuals generated by crossing top selections with elites.
            sampling_fraction (float): Fraction of individuals generated by sampling around population mean.
            elitism_fraction (float): Fraction of population preserved as elites.
            tournsize (int): Tournament size for selection.
        """
        diversity = self.calculate_diversity(population)
        if diversity >= diversity_threshold:
            return  # Diversity is sufficient; no action needed.
        # original_mutate = self.toolbox.mutate

        # Temporarily register the custom mutation
        self.toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.5, indpb=0.8)
        # Calculate the number of individuals for each strategy
        population_size = len(population)
        num_elites = int(population_size * elitism_fraction)
        num_mutate = int(population_size * mutate_fraction)
        num_crossover = int(population_size * crossover_fraction)
        num_sampling = int(population_size * sampling_fraction)

        # Sort population by fitness
        sorted_population = sorted(population, key=lambda ind: ind.fitness.values[0], reverse=True)
        elites = sorted_population[:num_elites]

        # **1. Aggressive Mutation of Top 30% Elites**
        mutated_population = []
        for elite in elites[:num_mutate]:
            mutated_individual = self.toolbox.clone(elite)
            self.toolbox.mutate(mutated_individual)  # Apply aggressive mutation
            mutated_population.append(mutated_individual)

        # **2. Tournament Selection for Sampling Around Population Mean**
        sampled_population = []
        for _ in range(num_sampling):
            parent = tools.selTournament(population, k=1, tournsize=tournsize)[0]
            sampled_individual = self.generate_individual_near_population(population)
            sampled_population.append(sampled_individual)

        # **3. Crossover Between Top 10% and Elites**
        crossover_population = []
        for _ in range(num_crossover):
            elite_parent = random.choice(elites)
            top_parent = random.choice(sorted_population[num_elites:num_elites + num_crossover])
            child = self.toolbox.clone(elite_parent)
            self.toolbox.mate(child, top_parent)  # Perform crossover
            crossover_population.append(child)
        
        # Restore the original mutation operator
        self.toolbox.unregister("mutate")  # Unregister the temporary mutation operator
        # self.toolbox.register("mutate", original_mutate)  # Re-register the original mutation operator

        print(f"Diversity reintroduced: {num_mutate} mutated, {num_sampling} sampled, {num_crossover} crossover individuals.")
        # Combine all strategies into the population
        # Calculate how many individuals to replace
        replace_count = int(len(population) * 0.5)  # Replace 50%
        retain_count = len(population) - replace_count

        # Sort by fitness and retain the top individuals
        sorted_population = sorted(population, key=lambda ind: ind.fitness.values[0], reverse=True)
        retained_individuals = sorted_population[:retain_count]

        # Combine retained individuals with new ones
        all_candidates = retained_individuals + mutated_population + sampled_population + crossover_population

        # Shuffle to randomize the order of candidates
        random.shuffle(all_candidates)

        # Ensure the population size remains constant
        population[:] = all_candidates[:len(population)]
        
    def generate_individual_near_population(self, population):
        """Generate a new individual near the population mean."""
        population_array = np.array([ind for ind in population])
        mean = np.mean(population_array, axis=0)
        std = np.std(population_array, axis=0)

        # Sample from a Gaussian distribution around the mean
        new_individual = np.random.normal(mean, std).tolist()
        new_individual = np.clip(new_individual, self.min_weight, self.max_weight)
        return creator.Individual(new_individual)
    def detect_and_refine_stagnation(self, best_individual, population, max_stagnation=5):
        """Apply local search if fitness improvement stagnates."""
        if best_individual.fitness.values[0] > self.best_fitness_so_far:
            self.best_fitness_so_far = best_individual.fitness.values[0]
            self.no_improvement_count = 0
        else:
            self.no_improvement_count += 1

        if self.no_improvement_count >= max_stagnation:
            print("Stagnation detected. Applying local search to refine best individuals.")
            top_individuals = tools.selBest(population, k=5)
            for ind in top_individuals:
                self.local_search(ind)
            self.no_improvement_count = 0

    def differential_evolution(self, population, fitness_dict, generation, total_generations, num_parents=5):
        """Perform Differential Evolution with custom number of parents."""
        F = 0.8  # Scaling factor
        CR = 0.9  # Crossover rate
        refined_population = population[:]
        for i, target in enumerate(population):
            idxs = [idx for idx in range(len(population)) if idx != i]
            if len(idxs) < num_parents:
                continue
            selected_indices = np.random.choice(idxs, num_parents, replace=False)
            donor = [
                np.mean([population[idx][j] for idx in selected_indices]) + F * np.random.randn()
                for j in range(len(target))
            ]
            trial = [
                donor[j] if np.random.rand() < CR else target[j]
                for j in range(len(target))
            ]
            trial_ind = creator.Individual(trial)
            trial_ind.fitness.values = self.toolbox.evaluate(trial_ind)
            if trial_ind.fitness.values[0] > target.fitness.values[0]:
                refined_population[i] = trial_ind
        return refined_population
    def _evaluate_population_with_cuda_streams(self, population, generation, total_generations):
        """
        Evaluate fitness of the population using CUDA streams, avoiding redundant computations.

        Args:
            population (list): The list of individuals to evaluate.
            generation (int): Current generation number.
            total_generations (int): Total number of generations.

        Returns:
            List of fitness values for the population.
        """
        streams = [torch.cuda.Stream(device=self.validation_class.device) for _ in range(len(population))]
        fitness_results = [None] * len(population)

        # Start evaluation in parallel streams
        for i, (ind, stream) in enumerate(zip(population, streams)):
            # Skip evaluation if fitness is valid
            if ind.fitness.valid:
                fitness_results[i] = ind.fitness.values[0]
            else:
                with torch.cuda.stream(stream):
                    weights = torch.FloatTensor(ind).reshape(self.last_layer_shape).to(self.validation_class.device)
                    fitness_results[i] = self.validation_class.validate_model_with_weights(weights, generation, total_generations)

        # Synchronize all streams
        torch.cuda.synchronize(self.validation_class.device)

        return fitness_results

    def run(self, population_size=50, num_generations=150, stop_threshold=95, save_interval=10, checkpoint_file="hybrid_cma_run3_savepoint.pt", diversity_threshold=0.01):
        """
        Run the hybrid CMA-ES + DE optimization process with diversity monitoring and maintenance.

        Args:
            population_size (int): Size of the population.
            num_generations (int): Number of generations to evolve.
            stop_threshold (float): Accuracy threshold to stop the optimization.
            save_interval (int): Interval (in generations) to save a checkpoint.
            checkpoint_file (str): Path to the checkpoint file.
            diversity_threshold (float): Minimum acceptable diversity. Below this, diversity is reintroduced.

        Returns:
            The best individual found during the optimization.
        """
        start_generation = 1
        refined_population = None 
        # Resume from checkpoint if requested and file exists
        if self.resume_from_checkpoint and os.path.exists(checkpoint_file):
            checkpoint = torch.load(checkpoint_file)
            self.cma_strategy = checkpoint["cma_strategy"]
            refined_population = checkpoint["population"]
            start_generation = checkpoint["generation"] + 1
            print(f"Resuming from generation {start_generation}.")

       # Initialize the refined population

        for generation in range(start_generation, num_generations + 1):
            print(f"Generation {generation}")

            # Use refined population if available, else generate a new population
            cma_population = refined_population if refined_population else self.toolbox.generate()

            # Evaluate fitness of CMA-ES population
            fitnesses = self._evaluate_population_with_cuda_streams(cma_population, generation, num_generations)
            for ind, fitness in zip(cma_population, fitnesses):
                ind.fitness.values = (fitness,)
            self.cma_strategy.update(cma_population)

            # Log the best individual in the CMA-ES population
            best_cma_ind = max(cma_population, key=lambda ind: ind.fitness.values[0])
            self.logger.info(f"Best CMA-ES accuracy: {best_cma_ind.fitness.values[0]:.2f}%")

            # Check stopping criterion
            if best_cma_ind.fitness.values[0] >= stop_threshold:
                print("Stopping criterion reached.")
                return best_cma_ind

            # Diversity Monitoring
            diversity = self.calculate_diversity(cma_population)
            self.logger.info(f"Population diversity at generation {generation}: {diversity:.4f}")
            print(f"Population diversity at generation {generation}: {diversity:.4f}")

            # Handle stagnation and diversity
            # self.detect_and_refine_stagnation(best_cma_ind, cma_population)

            if diversity < diversity_threshold:
                print("Diversity below threshold. Reintroducing diversity.")
                self.reintroduce_diversity_with_strategies(cma_population)

            # Apply Differential Evolution on top-performing individuals from CMA-ES
            top_cma_individuals = tools.selBest(cma_population, k=int(population_size * 0.3))  # Top 30%
            de_population = self.differential_evolution(top_cma_individuals, {}, generation, num_generations, num_parents=5)

            # Combine DE-refined population with remaining CMA-ES population
            refined_population = de_population + tools.selTournament(cma_population, k=int(population_size * 0.7), tournsize=3)

            # Save a checkpoint at the specified interval
            if generation % save_interval == 0:
                torch.save({
                    "cma_strategy": self.cma_strategy,
                    "generation": generation,
                    "population": refined_population,
                }, checkpoint_file)
                print(f"Checkpoint saved at generation {generation}.")

        # Return the best individual from the final refined population
        return tools.selBest(refined_population, k=1)[0]